\documentclass{article}
\usepackage{eecstex}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Expo}{\operatorname{Expo}}

\title{CS 70 HW 14}
\author{Bryan Ngo}
\date{2020-12-06}

\begin{document}

\maketitle

\section{Short Answer}

\subsection{}

\begin{align}
    X &= \mathcal{U}(0, 2) \\
    Y &= 2X + 1 \\
    f_X(x) &=
    \begin{cases}
        0 & x < 0 \\
        \frac{1}{2} & x \in (0, 2) \\
        0 & x > 2
    \end{cases} \\
    F_X(x) &= \int_0^x f_X(x) \, dx =
    \begin{cases}
        0 & x < 0 \\
        \frac{1}{2}x & x \in (0, 2) \\
        0 & x > 2
    \end{cases} \\
    f_Y(y) &= 2f_X(y) + 1 =
    \begin{cases}
        0 & y < 1 \\
        \frac{1}{4} & y \in (1, 5) \\
        0 & y > 5
    \end{cases} \\
    F_Y(y) &= \int_1^y f_Y(x) \, dx =
    \begin{cases}
        0 & y < 1 \\
        \frac{1}{4}(y - 1) & y \in (1, 5) \\
        0 & y > 5
    \end{cases} \\
    \E(X) &= 1 \\
    \E(Y) &= \E(2X + 1) = 2\E(X) + 1 = 3 \\
    \sigma^2(X) &= \frac{2^2}{12} = \frac{1}{3} \\
    \sigma^2(Y) &= \sigma^2(2X + 1) = 2^2 \sigma(X) = \frac{4}{3}
\end{align}

\subsection{}

\begin{equation}
    f(x, y) =
    \begin{cases}
        c x y + \frac{1}{4} & x \in [1, 2], y \in [0, 2] \\
        0 & \text{elsewhere}
    \end{cases}
\end{equation}
In order to find the constant, we set the total integral
\begin{align}
    \int_1^2 \int_0^2 c x y + \frac{1}{4} \, dy \, dx &= 1 \\
    &= \int_1^2 \left.\frac{1}{2} c x y^2 + \frac{1}{4} y\right|_{y = 0}^2 \, dx \\
    &= \int_1^2 2 c x + \frac{1}{2} \, dx \\
    &= \left.c x^2 + \frac{1}{2} x\right|_1^2 = 3c + \frac{1}{2} = 1 \\
    \Rightarrow c &= \frac{1}{6}
\end{align}
The marginal distribution \(f_X(x) = \frac{1}{3}x + \frac{1}{2}\).
Then, the conditional distribution
\begin{equation}
    f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)} = \frac{\frac{1}{6}xy + \frac{1}{4}}{\frac{1}{3}x + \frac{1}{2}} = \frac{2xy + 3}{2(2x + 3)}
\end{equation}
Since the probability is dependent on \(x\), they are not independent.

\section{Continuous Probablity Continued}

\subsection{}

Finding the expectation and variance,
\begin{align}
    \E(X_i) &= n (i \cdot \frac{1}{2} - i \frac{1}{2}) = 0 \\
    \sigma^2(X_i) &= n\E\left((X - \E(X))^2\right) = n\E(X^2) = n\left(i^2 \frac{1}{2} + i^2 \frac{1}{2}\right) = ni^2
\end{align}
Since every coin toss is i.i.d, then by the Central Limit Theorem,
\begin{equation}
    \lim_{n \to \infty} \frac{1}{\sqrt{n}} \sum_{i \in [1, k]} X_i = i \cdot \mathcal{N}(0, n i^2)
\end{equation}

\subsection{}

\begin{equation}
    \lim_{n \to \infty} \Pr\left(\sum_{i \in [1, n]} \frac{X_i - \mu}{\sigma n^\alpha} \in [-1, 1]\right) =
    \begin{cases}
        \Phi\left(\frac{1}{n^{\alpha + \frac{1}{2}}}\right) & \alpha < \frac{1}{2} \\
        \Phi(1) - \Phi(-1) & \alpha = \frac{1}{2} \\
        \Phi\left(\frac{1}{n^{\alpha - \frac{1}{2}}}\right) & \alpha > \frac{1}{2}
    \end{cases}
\end{equation}
% TODO: finish

\newpage
\section{Exponential Distributions: Lightbulbs}

\begin{equation}
    \lambda = \frac{1}{50}
\end{equation}

\begin{enumerate}
    \item The CDF of the exponential function is \(1 - e^{-\lambda x}\).
    Plugging in our parameter and timescale, we get \(\Pr(X \leqslant 30) = 1 - e^{-\frac{3}{5}}\).
    \item We take the complement of our CDF to get \(\Pr(X > 30) = e^{-\frac{3}{5}}\).
    \item Using the memoryless property of the exponential distribution,
    \(\Pr(X > 30 + 30 \mid X > 30) = \Pr(X > 30) = e^{-\frac{3}{5}}\).
\end{enumerate}

\section{Useful Uniforms}

\subsection{}

\(X \sim \mathcal{N}(0, 1)\) satisifies the properties and  \(X \sim \mathcal{U}(a, b)\) does not.

\subsection{}

\begin{proof}
    Given a continuous random variable \(X\) with \(\Pr(X \in (a, b)) > 0\) for all \(a, b \in \R\) and \(a \neq b\), \(F(x + \epsilon) > F(x)\) for all \(\epsilon > 0\), where \(F\) is the CDF.
\end{proof}
\begin{theorem}
    Our CDF \(\Pr(X \in (x, x + \epsilon)) = F(x + \epsilon) - F(x) > 0\), by definition of our distribution.
    Therefore, \(F(x + \epsilon) > F(x)\).
\end{theorem}
\begin{theorem}
    Given a distribution \(X\) as described above, \(F : \R \mapsto (0, 1)\) is invertible.
\end{theorem}
\begin{proof}
    To prove invertibility, we must prove a bijection between \(\R\) and the interval \((0, 1)\).
    Injectivity can be proven by the fact that our function is strictly increasing.
    Surjectivity can be proven since \(\lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty} F(x) = 1\).
    By the Intermediate Value Theorem, it must map to some value on the interval \((0, 1)\).
\end{proof}

\subsection{}

Since \(U \sim \mathcal{U}(0, 1)\), the distribution of \(F^{-1}(U)\) is a degenerate distribution with infinite interval and zero density.

\subsection{}

It would not be enough since we assumed continuity of our random variable in order to prove invertibility.
With a discrete variable, it may not necessarily be invertible.

\section{Uniform Means}

\subsection{}

The probability of any one being lower than a given rating \(x\) is \(1 - x\).
Using the tail sum formula and using the property of independent ratings,
\begin{align}
    \E(Y) &= \int_0^\infty \Pr(Y > y) \, dy = \int_0^1 \Pr(Y > y) \, dy \\
    &= \int_0^1 (1 - y)^n \, dy \\
    &= \left.-\frac{1}{n + 1} (1 - y)^{n + 1}\right|_0^1 = \frac{1}{n + 1}
\end{align}

\subsection{}

By definition of probability, we can use the fact that \(\E(Z) = 1 - \E(Y) = \frac{n}{n + 1}\).
Intuitively, this is because if one apple has the minimum score, the other apple has the maximum score.

\section{Darts but with ML}

\begin{enumerate}
    \item \(X \sim \mathcal{U}(0, 1)\), so \(f_X(x) =
    \begin{cases}
        1 & x \in [0, 1] \\
        0 & \text{elsewhere}     
    \end{cases}\).
    \item \(F_X(x) = \Pr(X > x) = x^2\), so the PDF \(f_X(x) = 2x\).
    \item Using the law of total probability, our \(f_{X}(x) =
    \begin{cases}
        p + (1 - p) 2x & x \in [0, 1] \\     
        (1 - p) 2x & \text{otherwise}    
    \end{cases}\).
\end{enumerate}

\stepcounter{subsection}
\stepcounter{subsection}
\stepcounter{subsection}

\subsection{}

By definition,
\begin{equation}
    \Pr(A \mid X \in [x, x + dx]) + \Pr(B \mid X \in [x, x + dx]) = 1
\end{equation}
Plugging into what is desired,
\begin{align}
    \Pr(A \mid X \in [x, x + dx]) &> 1 - \Pr(A \mid X \in [x, x + dx]) \\
    \Pr(A \mid X \in [x, x + dx]) &> \frac{1}{2} \\
    \frac{\Pr(X \in [x, x + dx] \mid A) \Pr(A)}{\Pr(X \in [x, x + dx])} &> \frac{1}{2} \\
    \frac{p^2 \, \cancel{dx}}{(p + (1 - p) 2x) \, \cancel{dx}} &> \frac{1}{2}
\end{align}
Solving for \(x\), we have
\begin{align}
    p + (1 - p) 2x &< 2p^2 \\
    \Rightarrow x &< \frac{p(2 - p)}{1 - p}
\end{align}

\section{Sampling a Gaussian with Uniform}

\subsection{}

\begin{theorem}
    Given a uniform random variable \(U_1 \sim \mathcal{U}(0, 1)\), \(-\ln(U_1) \sim \Expo(1)\).
\end{theorem}
\begin{proof}
    Taking the CDF of the uniform distribution,
    \begin{align}
        F_{U_1}(u) &=
        \begin{cases}
            u & u \in (0, 1) \\
            0 & \text{elsewhere}
        \end{cases} \\
        \Rightarrow \Pr(-\ln(U_1) \leqslant u) &= \Pr(U_1 > e^{-u}) = 1 - \Pr(U_1 < e^{-u}) = 1 - e^{-u} = \Expo(1)
    \end{align}
\end{proof}

\subsection{}

\begin{theorem}
    Given two \(N_1, N_2 \sim \mathcal{N}(0, 1)\), \(N_1^2 + N_2^2 \sim \Expo\left(\frac{1}{2}\right)\).
\end{theorem}
\begin{proof}
    Taking the PDF of the normal joint distribution,
    \begin{equation}
        f_{N_1, N_2}(n_1, n_2) = \frac{1}{2 \pi} e^{-\frac{1}{2}(n_1 + n_2)^2}
    \end{equation}
    If we take the double integral and convert to polar coordinates letting \(R^2 = N_1^2 + N_2^2\),
    \begin{align}
        \Pr(R \leqslant r) = \iint \frac{1}{2 \pi} e^{-\frac{1}{2}(n_1 + n_2)^2} \, dn_2 \, dn_1 &= \frac{1}{2 \pi} \int_0^{2\pi} \int_0^{\sqrt{R}} r e^{-\frac{1}{2}r^2} \, dr \, d\theta \\
        &= \frac{1}{2 \pi} \int_0^{2\pi} \left.-e^{-\frac{1}{2}r^2}\right|_0^{\sqrt{R}} = \frac{1}{2\pi} \int_0^{2\pi} 1 - e^{-\frac{1}{2}R} \, d\theta \\
        &= 1 - e^{-\frac{1}{2}R} = \Expo\left(\frac{1}{2}\right)
    \end{align}
\end{proof}

\subsection{}

For our angle \(U_2\), we can simply pick a uniformly distributed random variable on the interval \((0, 2\pi)\).
For our radius \(U_1\), we can take a uniform variable \(U_1\) on the interval \((0, 1)\), then transform it using the fact that \(-\ln(U_1) \sim \Expo(1)\), then take the square root.

\section{Sundry}

I worked on this homework by myself.

\end{document}
